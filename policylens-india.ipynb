{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.kaggle.com/code/yahayamkayode/fine-tuning-gemma2b-model-using-lora-and-keras?scriptVersionId=208046475\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "<center><h1>Fine-tuning Gemma 2 Model Using LoRA and Keras with Custom Datatset</h1></center>\n",
    "\n",
    "\n",
    "# Introduction\n",
    "\n",
    "> In this project, I developed PolicyLens-India, an advanced conversational AI chatbot capable of answering questions based on Indian parliamentary debates and policies. The model is fine-tuned using Gemma2_2b, leveraging a custom Q&A-style dataset created from Indian parliamentary debate documents spanning 2024.\n",
    "\n",
    "> The dataset includes 500 QA pairs derived from comprehensive debates covering legislative processes, policy discussions, and key national issues. The chatbot's performance was rigorously evaluated on 10 QA pairs generated from 2024 parliamentary debates, achieving over 90% accuracy.\n",
    "\n",
    "\n",
    "#### The following resources were acknowledged for the successful implementation of this project\n",
    "\n",
    "> 1. Fine-tune Gemma models in Keras using LoRA, Kaggle Code, https://www.kaggle.com/code/nilaychauhan/fine-tune-gemma-models-in-keras-using-lora (Version 1) \n",
    "> 2. Fine-tune Gemma using LoRA and Keras, https://www.kaggle.com/code/gpreda/fine-tune-gemma-using-lora-and-keras\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Gemma 2?\n",
    "\n",
    "> Gemma is a collection of lightweight, advanced open models developed by Google, leveraging the same research and technology behind the Gemini models. These models are text-to-text, decoder-only large language models available in English, with open weights provided for both pre-trained and instruction-tuned versions. Gemma models excel in a range of text generation tasks, such as question answering, summarization, and reasoning. Their compact size allows for deployment in resource-constrained environments like laptops, desktops, or personal cloud infrastructure, making state-of-the-art AI models more accessible and encouraging innovation for all. \n",
    "\n",
    "> Gemma 2 represent the 2nd generation of Gemma models. These models were trained on a dataset of text data that includes a wide variety of sources. The **27B** model was trained with **13 trillion** tokens, the **9B** model was trained with **8 trillion tokens**, and **2B** model was trained with **2 trillion** tokens. Here is a summary of their key components: \n",
    "\n",
    "> To learn more about Gemma 2, follow this link: [Gemma 2 Model Card](https://www.kaggle.com/models/google/gemma-2).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is LoRA?  \n",
    "\n",
    "> **LoRA** stands for **Low-Rank Adaptation**. It is a method used to fine-tune large language models (LLMs) by freezing the weights of the LLM and injecting trainable rank-decomposition matrices. The number of trainable parameters during fine-tunning will decrease therefore considerably. According to **LoRA** paper, this number decreases **10,000 times**, and the computational resources size decreases 3 times. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How we proceed?\n",
    "\n",
    "> For fine-tunning with LoRA, we will follow the steps:\n",
    "\n",
    "> 1. Install prerequisites\n",
    "> 2. Load and process the maize data for fine-tuning\n",
    "> 3. Initialize the code for Gemma causal language model (Gemma Causal LM)\n",
    "> 4. Perform fine-tuning\n",
    "> 5. Test the fine-tunned model with questions from the data used for fine-tuning and with aditional questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "\n",
    "\n",
    "## Install packages\n",
    "\n",
    "We start by installing `keras-nlp` and `keras` packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T09:41:04.331507Z",
     "iopub.status.busy": "2024-11-18T09:41:04.331098Z",
     "iopub.status.idle": "2024-11-18T09:42:29.042285Z",
     "shell.execute_reply": "2024-11-18T09:42:29.041314Z",
     "shell.execute_reply.started": "2024-11-18T09:41:04.331469Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras-core in /opt/conda/lib/python3.10/site-packages (0.1.7)\n",
      "Requirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from keras-core) (1.4.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from keras-core) (1.26.4)\n",
      "Requirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from keras-core) (13.7.1)\n",
      "Requirement already satisfied: namex in /opt/conda/lib/python3.10/site-packages (from keras-core) (0.0.8)\n",
      "Requirement already satisfied: h5py in /opt/conda/lib/python3.10/site-packages (from keras-core) (3.11.0)\n",
      "Requirement already satisfied: dm-tree in /opt/conda/lib/python3.10/site-packages (from keras-core) (0.1.8)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras-core) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras-core) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras-core) (0.1.2)\n",
      "Requirement already satisfied: jax in /opt/conda/lib/python3.10/site-packages (0.4.26)\n",
      "Requirement already satisfied: jaxlib in /opt/conda/lib/python3.10/site-packages (0.4.26.dev20240620)\n",
      "Requirement already satisfied: ml-dtypes>=0.2.0 in /opt/conda/lib/python3.10/site-packages (from jax) (0.3.2)\n",
      "Requirement already satisfied: numpy>=1.22 in /opt/conda/lib/python3.10/site-packages (from jax) (1.26.4)\n",
      "Requirement already satisfied: opt-einsum in /opt/conda/lib/python3.10/site-packages (from jax) (3.3.0)\n",
      "Requirement already satisfied: scipy>=1.9 in /opt/conda/lib/python3.10/site-packages (from jax) (1.14.1)\n",
      "Requirement already satisfied: keras-nlp in /opt/conda/lib/python3.10/site-packages (0.17.0)\n",
      "Requirement already satisfied: keras-hub==0.17.0 in /opt/conda/lib/python3.10/site-packages (from keras-nlp) (0.17.0)\n",
      "Requirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from keras-hub==0.17.0->keras-nlp) (1.4.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from keras-hub==0.17.0->keras-nlp) (1.26.4)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from keras-hub==0.17.0->keras-nlp) (21.3)\n",
      "Requirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from keras-hub==0.17.0->keras-nlp) (2024.5.15)\n",
      "Requirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from keras-hub==0.17.0->keras-nlp) (13.7.1)\n",
      "Requirement already satisfied: kagglehub in /opt/conda/lib/python3.10/site-packages (from keras-hub==0.17.0->keras-nlp) (0.3.4)\n",
      "Requirement already satisfied: tensorflow-text in /opt/conda/lib/python3.10/site-packages (from keras-hub==0.17.0->keras-nlp) (2.16.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from kagglehub->keras-hub==0.17.0->keras-nlp) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from kagglehub->keras-hub==0.17.0->keras-nlp) (4.66.4)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->keras-hub==0.17.0->keras-nlp) (3.1.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras-hub==0.17.0->keras-nlp) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras-hub==0.17.0->keras-nlp) (2.18.0)\n",
      "Requirement already satisfied: tensorflow<2.17,>=2.16.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow-text->keras-hub==0.17.0->keras-nlp) (2.16.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras-hub==0.17.0->keras-nlp) (0.1.2)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text->keras-hub==0.17.0->keras-nlp) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text->keras-hub==0.17.0->keras-nlp) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text->keras-hub==0.17.0->keras-nlp) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text->keras-hub==0.17.0->keras-nlp) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text->keras-hub==0.17.0->keras-nlp) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text->keras-hub==0.17.0->keras-nlp) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text->keras-hub==0.17.0->keras-nlp) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text->keras-hub==0.17.0->keras-nlp) (3.3.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text->keras-hub==0.17.0->keras-nlp) (3.20.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text->keras-hub==0.17.0->keras-nlp) (70.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text->keras-hub==0.17.0->keras-nlp) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text->keras-hub==0.17.0->keras-nlp) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text->keras-hub==0.17.0->keras-nlp) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text->keras-hub==0.17.0->keras-nlp) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text->keras-hub==0.17.0->keras-nlp) (1.62.2)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text->keras-hub==0.17.0->keras-nlp) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text->keras-hub==0.17.0->keras-nlp) (3.5.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text->keras-hub==0.17.0->keras-nlp) (0.37.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->kagglehub->keras-hub==0.17.0->keras-nlp) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->kagglehub->keras-hub==0.17.0->keras-nlp) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->kagglehub->keras-hub==0.17.0->keras-nlp) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->kagglehub->keras-hub==0.17.0->keras-nlp) (2024.8.30)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow<2.17,>=2.16.1->tensorflow-text->keras-hub==0.17.0->keras-nlp) (0.43.0)\n",
      "Requirement already satisfied: namex in /opt/conda/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow<2.17,>=2.16.1->tensorflow-text->keras-hub==0.17.0->keras-nlp) (0.0.8)\n",
      "Requirement already satisfied: optree in /opt/conda/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow<2.17,>=2.16.1->tensorflow-text->keras-hub==0.17.0->keras-nlp) (0.11.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16.1->tensorflow-text->keras-hub==0.17.0->keras-nlp) (3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16.1->tensorflow-text->keras-hub==0.17.0->keras-nlp) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16.1->tensorflow-text->keras-hub==0.17.0->keras-nlp) (3.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16.1->tensorflow-text->keras-hub==0.17.0->keras-nlp) (2.1.5)\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies\n",
    "!pip install -q -U wurlitzer\n",
    "!pip install keras-core\n",
    "!pip install -q -U keras-nlp\n",
    "!pip install -q -U keras==3.5.0  # Use Keras 3.x to work with JAX\n",
    "!pip install -q -U kagglehub --upgrade\n",
    "!pip install jax jaxlib\n",
    "!pip install keras-nlp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install keras-nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages\n",
    "\n",
    "Now we can import the packages we just installed. We will also install `os`, so that we can set the environment variables needed for keras backend. We will use `jax` as `KERAS_BACKEND`.\n",
    "\n",
    "Because we want to publish the Model from the Notebook, we also include `kagglehub` and import secrets from `Kaggle App`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T09:42:29.044720Z",
     "iopub.status.busy": "2024-11-18T09:42:29.044381Z",
     "iopub.status.idle": "2024-11-18T09:42:42.010100Z",
     "shell.execute_reply": "2024-11-18T09:42:42.009317Z",
     "shell.execute_reply.started": "2024-11-18T09:42:29.044687Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\" # you can also use tensorflow or torch\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"1.00\" # avoid memory fragmentation on JAX backend.\n",
    "os.environ[\"JAX_PLATFORMS\"] = \"\"\n",
    "import keras\n",
    "import keras_nlp\n",
    "import kagglehub\n",
    "\n",
    "\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "secret_value_0 = user_secrets.get_secret(\"KAGGLE_KEY\")\n",
    "secret_value_1 = user_secrets.get_secret(\"KAGGLE_USERNAME\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas() # progress bar for pandas\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations\n",
    "\n",
    "\n",
    "We use a `Config` class to group the information needed to control the fine-tuning process:\n",
    "* random seed \n",
    "* dataset path\n",
    "* preset - name of pretrained Gemma 2\n",
    "* sequence length - this is the maximum size of input sequence for training\n",
    "* batch size - size of the input batch in training, x 2 as two GPUs\n",
    "* lora rank - rank for LoRA, higher means more trainable parameters \n",
    "* learning rate used in the train\n",
    "* epochs - number of epochs for train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T09:42:42.011756Z",
     "iopub.status.busy": "2024-11-18T09:42:42.011269Z",
     "iopub.status.idle": "2024-11-18T09:42:42.019744Z",
     "shell.execute_reply": "2024-11-18T09:42:42.018997Z",
     "shell.execute_reply.started": "2024-11-18T09:42:42.011722Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    seed = 42\n",
    "\n",
    "    dataset_path = \"/kaggle/input/indian-parliamentary-debates-data-2024\"  # Use your dataset's Kaggle path\n",
    "    preset = \"gemma2_2b_en\" # name of pretrained Gemma 2\n",
    "    sequence_length = 512 # max size of input sequence for training\n",
    "    batch_size = 1 # size of the input batch in training\n",
    "    lora_rank = 4 # rank for LoRA, higher means more trainable parameters\n",
    "    learning_rate=8e-5 # learning rate used in train\n",
    "    epochs = 12 # number of epochs to train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T09:42:42.022983Z",
     "iopub.status.busy": "2024-11-18T09:42:42.022116Z",
     "iopub.status.idle": "2024-11-18T09:42:42.053316Z",
     "shell.execute_reply": "2024-11-18T09:42:42.052422Z",
     "shell.execute_reply.started": "2024-11-18T09:42:42.022934Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "keras.utils.set_random_seed(Config.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data\n",
    "\n",
    "\n",
    "We load the data we will use for fine-tunining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T09:42:42.055699Z",
     "iopub.status.busy": "2024-11-18T09:42:42.055406Z",
     "iopub.status.idle": "2024-11-18T09:42:42.091983Z",
     "shell.execute_reply": "2024-11-18T09:42:42.091106Z",
     "shell.execute_reply.started": "2024-11-18T09:42:42.055661Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Context</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>460</th>\n",
       "      <td>What did the Speaker emphasize about discussio...</td>\n",
       "      <td>The Speaker highlighted the need for discussio...</td>\n",
       "      <td>The Speaker emphasized that discussions should...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>How does the government plan to support domest...</td>\n",
       "      <td>The government plans to develop infrastructure...</td>\n",
       "      <td>To support domestic tourism growth, the govern...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>Who conveyed congratulations on behalf of the ...</td>\n",
       "      <td>Adv. Francis George from the Kerala Congress P...</td>\n",
       "      <td>Adv. Francis George congratulated Shri Om Birl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>Who is the member from Arani and in which lang...</td>\n",
       "      <td>Shri Tharaniventhan M.S. from Arani took the a...</td>\n",
       "      <td>Shri Tharaniventhan M.S. represents Arani and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>Who expressed concerns about passing Bills wit...</td>\n",
       "      <td>Adv. Francis George from Kerala Congress expre...</td>\n",
       "      <td>Adv. Francis George voiced concerns about Bill...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>What was Shri Rahul Gandhi  request related to...</td>\n",
       "      <td>Shri Rahul Gandhi wanted to address student is...</td>\n",
       "      <td>He requested a discussion focused on student i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>Who represents Thane and took the oath in Mara...</td>\n",
       "      <td>Shri Naresh Ganpat Mhaske from Thane took the ...</td>\n",
       "      <td>Shri Naresh Ganpat Mhaske represents Thane and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>How has the National Education Policy 2020 cha...</td>\n",
       "      <td>The National Education Policy 2020 introduces ...</td>\n",
       "      <td>The National Education Policy 2020 has brought...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Question  \\\n",
       "460  What did the Speaker emphasize about discussio...   \n",
       "73   How does the government plan to support domest...   \n",
       "231  Who conveyed congratulations on behalf of the ...   \n",
       "175  Who is the member from Arani and in which lang...   \n",
       "237  Who expressed concerns about passing Bills wit...   \n",
       "425  What was Shri Rahul Gandhi  request related to...   \n",
       "155  Who represents Thane and took the oath in Mara...   \n",
       "55   How has the National Education Policy 2020 cha...   \n",
       "\n",
       "                                               Context  \\\n",
       "460  The Speaker highlighted the need for discussio...   \n",
       "73   The government plans to develop infrastructure...   \n",
       "231  Adv. Francis George from the Kerala Congress P...   \n",
       "175  Shri Tharaniventhan M.S. from Arani took the a...   \n",
       "237  Adv. Francis George from Kerala Congress expre...   \n",
       "425  Shri Rahul Gandhi wanted to address student is...   \n",
       "155  Shri Naresh Ganpat Mhaske from Thane took the ...   \n",
       "55   The National Education Policy 2020 introduces ...   \n",
       "\n",
       "                                                Answer  \n",
       "460  The Speaker emphasized that discussions should...  \n",
       "73   To support domestic tourism growth, the govern...  \n",
       "231  Adv. Francis George congratulated Shri Om Birl...  \n",
       "175  Shri Tharaniventhan M.S. represents Arani and ...  \n",
       "237  Adv. Francis George voiced concerns about Bill...  \n",
       "425  He requested a discussion focused on student i...  \n",
       "155  Shri Naresh Ganpat Mhaske represents Thane and...  \n",
       "55   The National Education Policy 2020 has brought...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(f\"{Config.dataset_path}/PolicyData_2024_mini.csv\")\n",
    "df.sample(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For easiness, we will create the following template for QA: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T09:42:42.093780Z",
     "iopub.status.busy": "2024-11-18T09:42:42.093181Z",
     "iopub.status.idle": "2024-11-18T09:42:42.112903Z",
     "shell.execute_reply": "2024-11-18T09:42:42.111997Z",
     "shell.execute_reply.started": "2024-11-18T09:42:42.093737Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "template = \"\\n\\nQuestion:\\n{Question}\\n\\nAnswer:\\n{Answer}\"\n",
    "\n",
    "df[\"prompt\"] = df.apply(lambda row: template.format(Question=row.Question,\n",
    "                                                    Answer=row.Answer), axis=1)\n",
    "data = df.prompt.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Template utility function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T09:42:42.114104Z",
     "iopub.status.busy": "2024-11-18T09:42:42.113836Z",
     "iopub.status.idle": "2024-11-18T09:42:42.122205Z",
     "shell.execute_reply": "2024-11-18T09:42:42.121400Z",
     "shell.execute_reply.started": "2024-11-18T09:42:42.114074Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def colorize_text(text):\n",
    "    for word, color in zip([\"Question\", \"Answer\"], [\"red\", \"green\"]):\n",
    "        text = text.replace(f\"\\n\\n{word}:\", f\"\\n\\n**<font color='{color}'>{word}:</font>**\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specialized class to query Gemma\n",
    "\n",
    "\n",
    "We define a specialized class to query Gemma. But first, we need to initialize an object of GemmaCausalLM class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the code for Gemma Causal LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T09:42:53.717601Z",
     "iopub.status.busy": "2024-11-18T09:42:53.717255Z",
     "iopub.status.idle": "2024-11-18T09:42:53.723006Z",
     "shell.execute_reply": "2024-11-18T09:42:53.722022Z",
     "shell.execute_reply.started": "2024-11-18T09:42:53.717558Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17.0\n"
     ]
    }
   ],
   "source": [
    "import keras_nlp\n",
    "print(keras_nlp.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T09:42:53.728351Z",
     "iopub.status.busy": "2024-11-18T09:42:53.727923Z",
     "iopub.status.idle": "2024-11-18T09:43:47.300135Z",
     "shell.execute_reply": "2024-11-18T09:43:47.299193Z",
     "shell.execute_reply.started": "2024-11-18T09:42:53.728293Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    }
   ],
   "source": [
    "gemma_causal_lm = keras_nlp.models.GemmaCausalLM.from_preset(Config.preset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T09:43:47.301539Z",
     "iopub.status.busy": "2024-11-18T09:43:47.301237Z",
     "iopub.status.idle": "2024-11-18T09:43:47.333432Z",
     "shell.execute_reply": "2024-11-18T09:43:47.332544Z",
     "shell.execute_reply.started": "2024-11-18T09:43:47.301507Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                                                  </span>┃<span style=\"font-weight: bold\">                                   Config </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                              │                      Vocab size: <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n",
       "└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                                                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                  Config\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                              │                      Vocab size: \u001b[38;5;34m256,000\u001b[0m │\n",
       "└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2304</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">589,824,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2304\u001b[0m)        │   \u001b[38;5;34m2,614,341,888\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
       "│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m589,824,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> (9.74 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,614,341,888\u001b[0m (9.74 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> (9.74 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,614,341,888\u001b[0m (9.74 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gemma_causal_lm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the specialized class\n",
    "\n",
    "Here we define the special class `GemmaQA`. \n",
    "in the `__init__` we pass the `GemmaCausalLM` object created before.\n",
    "The `query` member function uses `GemmaCausalLM` member function `generate` to generate the answer, based on a prompt that includes the category and the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T09:43:47.334892Z",
     "iopub.status.busy": "2024-11-18T09:43:47.334595Z",
     "iopub.status.idle": "2024-11-18T09:43:47.340245Z",
     "shell.execute_reply": "2024-11-18T09:43:47.339342Z",
     "shell.execute_reply.started": "2024-11-18T09:43:47.334859Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class GemmaQA:\n",
    "    def __init__(self, max_length=512):\n",
    "        self.max_length = max_length\n",
    "        self.prompt = template\n",
    "        self.gemma_causal_lm = gemma_causal_lm\n",
    "        \n",
    "    def query(self, question):\n",
    "        response = self.gemma_causal_lm.generate(\n",
    "            self.prompt.format(\n",
    "                Question=question,\n",
    "                Answer=\"\"), \n",
    "            max_length=self.max_length)\n",
    "        display(Markdown(colorize_text(response)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gemma preprocessor\n",
    "\n",
    "\n",
    "This preprocessing layer will take in batches of strings, and return outputs in a ```(x, y, sample_weight)``` format, where the y label is the next token id in the x sequence.\n",
    "\n",
    "From the code below, we can see that, after the preprocessor, the data shape is ```(num_samples, sequence_length)```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T09:43:47.341798Z",
     "iopub.status.busy": "2024-11-18T09:43:47.341437Z",
     "iopub.status.idle": "2024-11-18T09:43:47.442733Z",
     "shell.execute_reply": "2024-11-18T09:43:47.441825Z",
     "shell.execute_reply.started": "2024-11-18T09:43:47.341756Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "x, y, sample_weight = gemma_causal_lm.preprocessor(data[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform fine-tuning with LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enable LoRA for the model\n",
    "\n",
    "LoRA rank is setting the number of trainable parameters. A larger rank will result in a larger number of parameters to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T09:43:47.444296Z",
     "iopub.status.busy": "2024-11-18T09:43:47.443988Z",
     "iopub.status.idle": "2024-11-18T09:43:47.737386Z",
     "shell.execute_reply": "2024-11-18T09:43:47.736497Z",
     "shell.execute_reply.started": "2024-11-18T09:43:47.444265Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                                                  </span>┃<span style=\"font-weight: bold\">                                   Config </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                              │                      Vocab size: <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n",
       "└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                                                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                  Config\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                              │                      Vocab size: \u001b[38;5;34m256,000\u001b[0m │\n",
       "└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2304</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,617,270,528</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">589,824,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2304\u001b[0m)        │   \u001b[38;5;34m2,617,270,528\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
       "│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m589,824,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,617,270,528</span> (9.75 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,617,270,528\u001b[0m (9.75 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,928,640</span> (11.17 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,928,640\u001b[0m (11.17 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> (9.74 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,614,341,888\u001b[0m (9.74 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Enable LoRA for the model and set the LoRA rank to the lora_rank as set in Config (4).\n",
    "gemma_causal_lm.backbone.enable_lora(rank=Config.lora_rank)\n",
    "gemma_causal_lm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the training sequence\n",
    "\n",
    "We set the `sequence_length` for the `GemmaCausalLM` (from configuration, will be 512).\n",
    "We compile the model, with the loss, optimizer and metric.\n",
    "For the metric, it is used `SparseCategoricalAccuracy`. This metric calculates how often predictions match integer labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T09:43:47.738815Z",
     "iopub.status.busy": "2024-11-18T09:43:47.738521Z",
     "iopub.status.idle": "2024-11-18T11:06:47.008453Z",
     "shell.execute_reply": "2024-11-18T11:06:47.007559Z",
     "shell.execute_reply.started": "2024-11-18T09:43:47.738783Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m459s\u001b[0m 855ms/step - loss: 0.1931 - sparse_categorical_accuracy: 0.6039\n",
      "Epoch 2/12\n",
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m427s\u001b[0m 819ms/step - loss: 0.1353 - sparse_categorical_accuracy: 0.6834\n",
      "Epoch 3/12\n",
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m409s\u001b[0m 820ms/step - loss: 0.1244 - sparse_categorical_accuracy: 0.7014\n",
      "Epoch 4/12\n",
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m409s\u001b[0m 820ms/step - loss: 0.1145 - sparse_categorical_accuracy: 0.7190\n",
      "Epoch 5/12\n",
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m409s\u001b[0m 820ms/step - loss: 0.1037 - sparse_categorical_accuracy: 0.7405\n",
      "Epoch 6/12\n",
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m409s\u001b[0m 820ms/step - loss: 0.0925 - sparse_categorical_accuracy: 0.7658\n",
      "Epoch 7/12\n",
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m409s\u001b[0m 820ms/step - loss: 0.0805 - sparse_categorical_accuracy: 0.7891\n",
      "Epoch 8/12\n",
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m409s\u001b[0m 820ms/step - loss: 0.0685 - sparse_categorical_accuracy: 0.8160\n",
      "Epoch 9/12\n",
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m409s\u001b[0m 820ms/step - loss: 0.0576 - sparse_categorical_accuracy: 0.8434\n",
      "Epoch 10/12\n",
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m409s\u001b[0m 820ms/step - loss: 0.0490 - sparse_categorical_accuracy: 0.8646\n",
      "Epoch 11/12\n",
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m409s\u001b[0m 819ms/step - loss: 0.0418 - sparse_categorical_accuracy: 0.8869\n",
      "Epoch 12/12\n",
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m409s\u001b[0m 819ms/step - loss: 0.0362 - sparse_categorical_accuracy: 0.9003\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7f90fc538550>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#set sequence length cf. config (512)\n",
    "gemma_causal_lm.preprocessor.sequence_length = Config.sequence_length \n",
    "\n",
    "# Compile the model with loss, optimizer, and metric\n",
    "gemma_causal_lm.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=Config.learning_rate),\n",
    "    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "\n",
    "# Train model\n",
    "gemma_causal_lm.fit(data, epochs=Config.epochs, batch_size=Config.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the fine-tuned model\n",
    "\n",
    "We instantiate an object of class GemmaQA. Because `gemma_causal_lm` was fine-tuned using LoRA, `gemma_qa` defined here will use the fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T11:06:47.010535Z",
     "iopub.status.busy": "2024-11-18T11:06:47.009836Z",
     "iopub.status.idle": "2024-11-18T11:06:47.015064Z",
     "shell.execute_reply": "2024-11-18T11:06:47.014100Z",
     "shell.execute_reply.started": "2024-11-18T11:06:47.010486Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "gemma_qa = GemmaQA()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For start, we will testing the model with some of the data from the training set itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T11:06:47.016776Z",
     "iopub.status.busy": "2024-11-18T11:06:47.016489Z",
     "iopub.status.idle": "2024-11-18T11:07:03.617628Z",
     "shell.execute_reply": "2024-11-18T11:07:03.616687Z",
     "shell.execute_reply.started": "2024-11-18T11:06:47.016746Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='red'>Question:</font>**\n",
       "What is the goal of the 'Sabka Saath, Sabka Vikas' philosophy?\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "'Sabka Saath, Sabka Vikas' is a philosophy that emphasizes inclusive growth, intending to reach every social and geographical section in India, and foster equal opportunities and prosperity for all segments of society."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "row = df.iloc[0]\n",
    "gemma_qa.query(row.Question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T11:07:03.619255Z",
     "iopub.status.busy": "2024-11-18T11:07:03.618883Z",
     "iopub.status.idle": "2024-11-18T11:07:05.270910Z",
     "shell.execute_reply": "2024-11-18T11:07:05.269961Z",
     "shell.execute_reply.started": "2024-11-18T11:07:03.619220Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='red'>Question:</font>**\n",
       "What is the PM-KISAN SAMMAN Yojana, and who benefits from it?\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "PM-KISAN SAMMAN Yojana is a program that provides direct financial assistance to 11.8 crore farmers, focusing on supporting small and marginal farmers."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "row = df.iloc[3]\n",
    "gemma_qa.query(row.Question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T11:07:05.272830Z",
     "iopub.status.busy": "2024-11-18T11:07:05.272167Z",
     "iopub.status.idle": "2024-11-18T11:07:06.249865Z",
     "shell.execute_reply": "2024-11-18T11:07:06.248914Z",
     "shell.execute_reply.started": "2024-11-18T11:07:05.272784Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='red'>Question:</font>**\n",
       "Who took the oath in Manipuri on June 25, 2024?\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "Shri Angomcha Bimol Akoijam took the oath in Manipuri."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "row = df.iloc[105]\n",
    "gemma_qa.query(row.Question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model with unseen question(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T11:15:48.595275Z",
     "iopub.status.busy": "2024-11-18T11:15:48.594524Z",
     "iopub.status.idle": "2024-11-18T11:15:49.613366Z",
     "shell.execute_reply": "2024-11-18T11:15:49.612418Z",
     "shell.execute_reply.started": "2024-11-18T11:15:48.595237Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='red'>Question:</font>**\n",
       "How is the government promoting inclusive education for the disabled?\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "Inclusive education promotes equal access for disabled students, with focus on modern facilities and resources."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = \"How is the government promoting inclusive education for the disabled?\"\n",
    "gemma_qa.query(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T12:17:33.135521Z",
     "iopub.status.busy": "2024-11-18T12:17:33.134871Z",
     "iopub.status.idle": "2024-11-18T12:17:34.759590Z",
     "shell.execute_reply": "2024-11-18T12:17:34.758601Z",
     "shell.execute_reply.started": "2024-11-18T12:17:33.135479Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='red'>Question:</font>**\n",
       "How has India promoted renewable energy?\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "India promotes renewable energy, aiming for 5 gigawatts of wind energy and 10 gigawatts of solar energy by 2027."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = \"How has India promoted renewable energy?\"\n",
    "gemma_qa.query(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T12:17:40.400170Z",
     "iopub.status.busy": "2024-11-18T12:17:40.399784Z",
     "iopub.status.idle": "2024-11-18T12:17:41.595095Z",
     "shell.execute_reply": "2024-11-18T12:17:41.594203Z",
     "shell.execute_reply.started": "2024-11-18T12:17:40.400115Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='red'>Question:</font>**\n",
       "What disaster relief funds are available to Tamil Nadu?\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "The government has released Rs. 1,111 crore from the NDRF for Tamil Nadu."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = \"What disaster relief funds are available to Tamil Nadu?\"\n",
    "gemma_qa.query(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T12:17:47.101454Z",
     "iopub.status.busy": "2024-11-18T12:17:47.101080Z",
     "iopub.status.idle": "2024-11-18T12:17:48.080271Z",
     "shell.execute_reply": "2024-11-18T12:17:48.079367Z",
     "shell.execute_reply.started": "2024-11-18T12:17:47.101421Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='red'>Question:</font>**\n",
       "What is the current disaster relief fund status for Tamil Nadu?\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "The government has released partial aid for Tamil Nadu's relief and restoration efforts."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = \"What is the current disaster relief fund status for Tamil Nadu?\"\n",
    "gemma_qa.query(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T11:26:46.818705Z",
     "iopub.status.busy": "2024-11-18T11:26:46.817840Z",
     "iopub.status.idle": "2024-11-18T11:26:48.175690Z",
     "shell.execute_reply": "2024-11-18T11:26:48.174767Z",
     "shell.execute_reply.started": "2024-11-18T11:26:46.818664Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='red'>Question:</font>**\n",
       "Who congratulated Shri Om Birla on behalf of the INDIA Alliance?\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "Shri Rahul Gandhi congratulated Shri Om Birla on behalf of the INDIA Alliance, highlighting the importance of a non-partisan approach."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = \"Who congratulated Shri Om Birla on behalf of the INDIA Alliance?\"\n",
    "gemma_qa.query(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T11:15:26.187073Z",
     "iopub.status.busy": "2024-11-18T11:15:26.186658Z",
     "iopub.status.idle": "2024-11-18T11:15:27.101902Z",
     "shell.execute_reply": "2024-11-18T11:15:27.100904Z",
     "shell.execute_reply.started": "2024-11-18T11:15:26.187026Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='red'>Question:</font>**\n",
       "What is the focus of India  National Solar Mission?\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "India  National Solar Mission aims to expand solar energy adoption across sectors."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = \"What is the focus of India  National Solar Mission?\"\n",
    "gemma_qa.query(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T11:15:17.283991Z",
     "iopub.status.busy": "2024-11-18T11:15:17.283268Z",
     "iopub.status.idle": "2024-11-18T11:15:18.313892Z",
     "shell.execute_reply": "2024-11-18T11:15:18.312935Z",
     "shell.execute_reply.started": "2024-11-18T11:15:17.283949Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='red'>Question:</font>**\n",
       "How is the Ministry of Health addressing the shortage of healthcare professionals?\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "The Ministry is addressing the shortage by increasing medical and para-medical education seats."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = \"How is the Ministry of Health addressing the shortage of healthcare professionals?\"\n",
    "gemma_qa.query(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T11:07:15.452254Z",
     "iopub.status.busy": "2024-11-18T11:07:15.451907Z",
     "iopub.status.idle": "2024-11-18T11:07:57.514292Z",
     "shell.execute_reply": "2024-11-18T11:07:57.513187Z",
     "shell.execute_reply.started": "2024-11-18T11:07:15.452215Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "preset_dir = \".\\gemma2_2b_en_policylens_model\"\n",
    "gemma_causal_lm.save_to_preset(preset_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Fine-tuning of a **Gemma 2** model has been demonstated using LoRA.   \n",
    "> -  A class was alos created to run queries to the **Gemma 2** model and tested it with some examples from the existing training data but also with some new, unseen questions.   \n",
    "> - The models was as a Keras model.\n",
    "> - The model was evaluated using Perplexity,recorded Perplexity value of 2.502. \n",
    "> - Finnally, the model was published as a Kaggle Model on Kaggle Models platform."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6109762,
     "sourceId": 9938431,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 78150,
     "modelInstanceId": 72244,
     "sourceId": 85984,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
